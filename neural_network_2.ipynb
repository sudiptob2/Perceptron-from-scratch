{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Coding a single layer perceptron (ANN) from scratch", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np\n\n# sigmoid function\ndef sigmoid(x,deriv=False):\n    if(deriv==True):\n        return x*(1-x) #Proof : https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e\n    return 1/(1+np.exp(-x))"
        }, 
        {
            "execution_count": 31, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# input dataset\nno_of_features = 4\nX = np.array([  [0,0,1],\n                [0,1,1],\n                [1,0,1],\n                [1,1,1] ])\n    \n# output dataset            \ny = np.array([[0,0,1,1]]).T"
        }, 
        {
            "execution_count": 32, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "       Prediction      Actual\nX 0   [0.00966258]     [0]\nX 1   [0.00786349]     [0]\nX 2   [0.99358996]     [1]\nX 3   [0.99212165]     [1]\n\nPrevious Weight Updated Weight:\n-0.16596         9.67350\n0.44065         -0.20775\n-0.99977         -4.62984\n"
                }
            ], 
            "source": "#Training the neural network\n\n# seed random numbers to make calculation\n# seed is used to generate same random number at every run. \nnp.random.seed(1)\n\n# initialize weights\nw =  2*np.random.random((len(X[0]),1)) - 1\n\nw_copy = np.copy(w)\n\nfor iter in range(10000):\n\n    # forward propagation\n    # X is (4x3) matrix here\n    # Here weight is a (3x1) matrix\n    # We have to multipy feature set of each example with the weights.\n    # We have to do this for every sample\n    # Then for every training sample we will get a prediction/guess\n    # Here total training sample is 4\n    # So we will get 4 guess in a (4x1) vector\n    # Then we apply sigmoid to get output as a probability distribution\n    \n    S = np.empty([len(X),1]) # Hold the output of the first layer\n    y_pred = np.empty([len(X),1]) # Holds the probability of the outputs.\n    \n    for i in range(len(X)): #This len(X) returns the number of rows in X\n        sum = 0\n        for j in range(len(X[i])): #This len(X[i]) returns the number of colums in X\n            sum = sum + X[i,j]*w[j]\n        \n        # Now in the sum variable output for the first training example exist for ith iteration\n        # We apply sigmoid function to convert it into probability distribution\n        # Then append the output in the vector y_pred\n        # y_pred holds all the predictions after ith iteration\n        # print(sum)\n        S[i] = sum\n        y_pred[i] = sigmoid(sum) # y_pred holds f(S), Here f is sigmoid\n        \n        # Now for the first eaxmple we have to update all the weight corrsponding to all the features\n        # Now we have to update the weight according to the prediction of first iteration which is in y_pred\n        # Formula : w'(i) = w(i) + x(i) * learning_rate * error * derivative(f(S))\n        # Here I uses sigmoid function as the nonlinear function. So f is sigmoid\n        # learning_rate = 1\n        # Do this for all the training example\n        for j in range(len(X[i])):\n            w[j] = w[j] + X[i,j]* 1 * (y[i]-y_pred[i]) * sigmoid(sigmoid(S[i]),True)\n        \n        # Now in the next iteration do all this again\n        \n# PRINTING THE OUTPUT\nprint(\"       Prediction      Actual\")\nfor i in range(len(y)):\n    print(\"X\",i,\" \",y_pred[i], \"   \", y[i])\nprint(\"\\nPrevious Weight\",\"Updated Weight:\")\nfor i in range(len(w)):\n    print(\"%5.5f         %5.5f\"%(w_copy[i],w[i]))"
        }, 
        {
            "source": "References: 1. Derivation and definitations followed: https://www.neural-networks.io/en/single-layer/learning-rule-demonstration.php\n            2. Derivative of sigmoid proof : https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e\n            3. Dataset and first idea: https://iamtrask.github.io/2015/07/12/basic-python-network/", 
            "cell_type": "raw", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}